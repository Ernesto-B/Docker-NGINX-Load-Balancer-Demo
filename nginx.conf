events {}   # Use default settings

http {
    # Enable gzip compression for faster content delivery. Not on every format bc compressing requires resources
    gzip on;
    gzip_types text/plain application/json text/css application/javascript;

    # Rate limiting: 2 requests per second per client IP, and calling this rate limiter "mylimit"
    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=2r/s;

    # Cache config for cache "my_cache". Defines cache path (made inside container) and parameters
    # Use levels to avoid having too many files in a single dir. for performance & scalability
    proxy_cache_path /tmp/nginx_cache levels=1:2 keys_zone=my_cache:1m inactive=1m use_temp_path=off;

    # Where to load balance between, using round-robin by default
    upstream backend {
        server node-app1:3000;
        server node-app2:3000;
    }

    # I intentionally set it to not load balance for the /info resource
    # Automatically detect and remove unhealthy backend servers from the load balancing pool
    upstream backendInfo {
        # max_fails is how many times the server connection can fail
        # fail_timeout is how long the server will be marked as down for if max_fails triggers
        server node-app1:3000 max_fails=2 fail_timeout=30s;
    }

    server {
        listen 80;

        # Forward req to root URL towards the backend upstream above
        location / {
            limit_req zone=mylimit burst=3 nodelay;    # Apply rate limiting according to the mylimit zone defined above. Allow 10 excess requests (over limit) that get queued. if queue is full, excess requests are dropped 
            proxy_cache my_cache;   # Enable caching according to the my_cache zone described above
            proxy_cache_valid 200 1m;   # Cache 200 OK responses for 1 minute, then they will be updated
            proxy_pass http://backend;  # Forward requests to the backend upstream
        }

        location /info {
            proxy_pass http://backendInfo;
        }
    }
}
